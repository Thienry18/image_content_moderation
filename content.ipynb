{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saran Optimasi untuk Model Klasifikasi Konten\n",
    "\n",
    "Notebook ini berisi saran-saran yang telah dianalisis untuk meningkatkan akurasi model klasifikasi konten Anda, berdasarkan notebook `content_moderation_full2.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Impor Library dan Konfigurasi Awal\n",
    "Pastikan semua library yang dibutuhkan telah diimpor dan konfigurasi awal seperti seed dan mixed precision telah diatur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import EfficientNetB2 # Atau EfficientNetB0, sesuaikan\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input # Sesuaikan jika base model berbeda\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import mixed_precision\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "\n",
    "# Set seed untuk reproduktifitas\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Mixed Precision (jika menggunakan GPU yang mendukung dan TF >= 2.4)\n",
    "# mixed_precision.set_global_policy('mixed_float16') \n",
    "# Catatan: Output dari notebook Anda menunjukkan tidak ada GPU yang tersedia, \n",
    "# jadi mixed precision mungkin tidak memberikan manfaat signifikan dan bisa diabaikan jika di CPU.\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Konfigurasi Dataset dan Parameter Model\n",
    "Definisikan path dataset, ukuran gambar, batch size, dan nama kelas. Pastikan `img_size` sesuai dengan model dasar yang dipilih (misalnya, EfficientNetB2 umumnya menggunakan (260, 260))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model: EfficientNetB2\n",
      "Image size: (260, 260)\n",
      "Batch size: 32\n",
      "Number of classes: 9\n",
      "Classes: ['Accident', 'Blood', 'Blood and Gore', 'Explosion', 'Normal', 'Sexual Harassment', 'Suicide', 'Violence', 'nudity']\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'converted/train'\n",
    "val_dir = 'converted/valid'\n",
    "test_dir = 'converted/test'\n",
    "\n",
    "# Pilih base model dan sesuaikan img_size\n",
    "BASE_MODEL_CHOICE = EfficientNetB2 # Ganti ke EfficientNetB0 jika itu yang Anda gunakan\n",
    "\n",
    "if BASE_MODEL_CHOICE == EfficientNetB2:\n",
    "    img_size = (260, 260)\n",
    "    PREPROCESS_INPUT_FUNC = tf.keras.applications.efficientnet.preprocess_input\n",
    "else: # Asumsi EfficientNetB0 atau default lain\n",
    "    img_size = (224, 224)\n",
    "    PREPROCESS_INPUT_FUNC = tf.keras.applications.efficientnet.preprocess_input # atau sesuaikan\n",
    "\n",
    "batch_size = 32 # atau 64, bisa dieksperimen\n",
    "class_names = sorted(os.listdir(train_dir))\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"Base Model: {BASE_MODEL_CHOICE.__name__}\")\n",
    "print(f\"Image size: {img_size}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(\"Classes:\", class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Generators dengan Augmentasi\n",
    "Augmentasi data penting untuk mencegah overfitting dan meningkatkan generalisasi model. Sesuaikan parameter augmentasi jika diperlukan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3600 images belonging to 9 classes.\n",
      "Found 919 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "def create_data_generators(img_size_param, batch_size_param, train_dir_param, val_dir_param):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.15,\n",
    "        height_shift_range=0.15,\n",
    "        zoom_range=0.15,\n",
    "        horizontal_flip=True,\n",
    "        brightness_range=[0.8, 1.2],\n",
    "        shear_range=0.1\n",
    "    )\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir_param,\n",
    "        target_size=img_size_param,\n",
    "        batch_size=batch_size_param,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        val_dir_param,\n",
    "        target_size=img_size_param,\n",
    "        batch_size=batch_size_param,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "    return train_generator, val_generator\n",
    "\n",
    "train_generator, val_generator = create_data_generators(img_size, batch_size, train_dir, val_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Penanganan Ketidakseimbangan Kelas\n",
    "Ketidakseimbangan kelas dapat mempengaruhi performa model. Pertimbangkan untuk menggunakan class weights standar atau Focal Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Balanced Class Weights: {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0, 7: 1.0, 8: 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Opsi 1: Sklearn balanced class weights\n",
    "y_train_classes = train_generator.classes\n",
    "sklearn_class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_classes),\n",
    "    y=y_train_classes\n",
    ")\n",
    "sklearn_class_weights_dict = dict(enumerate(sklearn_class_weights))\n",
    "print(\"Sklearn Balanced Class Weights:\", sklearn_class_weights_dict)\n",
    "\n",
    "# Opsi 2: Fungsi custom class weights Anda (pastikan terdefinisi)\n",
    "# def calculate_custom_class_weights(y_train, alpha=0.5, num_classes_param=num_classes):\n",
    "#     class_counts = Counter(y_train)\n",
    "#     total_samples = len(y_train)\n",
    "#     weights = {}\n",
    "#     for class_idx, count in class_counts.items():\n",
    "#         standard_weight = total_samples / (num_classes_param * count)\n",
    "#         if count > total_samples * 0.4: \n",
    "#             penalty_weight = standard_weight * (1 + alpha * (count / total_samples))\n",
    "#             weights[class_idx] = 1.0 / penalty_weight\n",
    "#         else:\n",
    "#             weights[class_idx] = standard_weight\n",
    "#     return weights\n",
    "# custom_weights = calculate_custom_class_weights(y_train_classes, alpha=0.5)\n",
    "# print(\"Custom Class Weights (alpha=0.5):\", custom_weights)\n",
    "\n",
    "# Pilih class weights yang akan digunakan:\n",
    "active_class_weights = sklearn_class_weights_dict # atau custom_weights\n",
    "\n",
    "# Opsi 3: Focal Loss (fungsi didefinisikan di bawah)\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        ce = -y_true * tf.math.log(y_pred)\n",
    "        weight = alpha * y_true * tf.pow((1 - y_pred), gamma)\n",
    "        fl = weight * ce\n",
    "        return tf.reduce_mean(tf.reduce_sum(fl, axis=1))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pembuatan Model\n",
    "Gunakan model dasar pre-trained dan tambahkan layer kustom di atasnya. Sesuaikan jumlah layer yang di-fine-tune dan parameter regularisasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\S8NRC\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_selected_model(num_classes_param, img_size_param):\n",
    "    base_model = BASE_MODEL_CHOICE(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=img_size_param + (3,),\n",
    "        pooling='avg'\n",
    "    )\n",
    "    \n",
    "    # Sesuaikan jumlah layer yang di-fine-tune\n",
    "    num_fine_tune_layers = 30 # Untuk B2, mungkin perlu disesuaikan untuk B0 atau model lain\n",
    "    if len(base_model.layers) > num_fine_tune_layers:\n",
    "        for layer in base_model.layers[:-num_fine_tune_layers]:\n",
    "            layer.trainable = False\n",
    "        for layer in base_model.layers[-num_fine_tune_layers:]:\n",
    "            # Pastikan layer tidak BatchNormalization sebelum dijadikan trainable jika mixed precision aktif\n",
    "            if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "                 layer.trainable = True\n",
    "    else: # Jika model lebih kecil dari num_fine_tune_layers, fine-tune semua\n",
    "        for layer in base_model.layers:\n",
    "            if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "                 layer.trainable = True\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=img_size_param + (3,)),\n",
    "        layers.Lambda(PREPROCESS_INPUT_FUNC),\n",
    "        base_model,\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(num_classes_param, activation='softmax', dtype='float32') # Output float32 jika pakai mixed precision\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = create_selected_model(num_classes, img_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Callbacks dan Kompilasi Model\n",
    "Gunakan callbacks seperti `ReduceLROnPlateau`, `EarlyStopping`, `ModelCheckpoint`, dan `WarmUpLearningRateScheduler` yang dimodifikasi. Pilih loss function dan optimizer yang sesuai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ efficientnetb2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1408</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,768,569</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1408</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">360,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,313</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m260\u001b[0m, \u001b[38;5;34m260\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ efficientnetb2 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1408\u001b[0m)           │     \u001b[38;5;34m7,768,569\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1408\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m360,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │         \u001b[38;5;34m2,313\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,131,586</span> (31.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,131,586\u001b[0m (31.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,335,189</span> (12.72 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,335,189\u001b[0m (12.72 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,796,397</span> (18.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,796,397\u001b[0m (18.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "INITIAL_LR = 1e-4 # Eksperimen dengan 1e-4 atau 5e-5\n",
    "WARMUP_EPOCHS = 5\n",
    "BASE_LR_WARMUP = 1e-7 # LR awal yang sangat kecil untuk warmup\n",
    "\n",
    "class WarmUpLearningRateScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, target_lr, warmup_epochs, base_lr=1e-7, verbose=0):\n",
    "        super().__init__()\n",
    "        self.target_lr = target_lr\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.base_lr = base_lr\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch < self.warmup_epochs:\n",
    "            lr = self.base_lr + (self.target_lr - self.base_lr) * (epoch + 1) / self.warmup_epochs\n",
    "            if hasattr(self.model.optimizer, 'lr') and isinstance(self.model.optimizer.lr, tf.Variable):\n",
    "                self.model.optimizer.lr.assign(lr)\n",
    "            elif hasattr(self.model.optimizer, 'learning_rate') and isinstance(self.model.optimizer.learning_rate, tf.Variable):\n",
    "                self.model.optimizer.learning_rate.assign(lr)\n",
    "            if self.verbose > 0:\n",
    "                print(f\"\\nEpoch {epoch+1}: Warmup learning rate set to {lr:.7f}\")\n",
    "        elif epoch == self.warmup_epochs:\n",
    "             if hasattr(self.model.optimizer, 'lr') and isinstance(self.model.optimizer.lr, tf.Variable):\n",
    "                self.model.optimizer.lr.assign(self.target_lr)\n",
    "             elif hasattr(self.model.optimizer, 'learning_rate') and isinstance(self.model.optimizer.learning_rate, tf.Variable):\n",
    "                self.model.optimizer.learning_rate.assign(self.target_lr)\n",
    "             if self.verbose > 0:\n",
    "                print(f\"\\nEpoch {epoch+1}: Learning rate set to target {self.target_lr:.7f} after warmup.\")\n",
    "\n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-8, verbose=1)\n",
    "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "model_checkpoint_cb = ModelCheckpoint('best_model_optimized.keras', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "warmup_scheduler_cb = WarmUpLearningRateScheduler(target_lr=INITIAL_LR, warmup_epochs=WARMUP_EPOCHS, base_lr=BASE_LR_WARMUP, verbose=1)\n",
    "\n",
    "callbacks_list = [reduce_lr_cb, early_stopping_cb, model_checkpoint_cb, warmup_scheduler_cb]\n",
    "\n",
    "# Pilih loss function:\n",
    "# chosen_loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\n",
    "# use_class_weights_in_fit = True\n",
    "\n",
    "chosen_loss = focal_loss(gamma=2.0, alpha=0.25) # Eksperimen dengan parameter focal loss\n",
    "use_class_weights_in_fit = False # Umumnya tidak perlu dengan focal loss, tapi bisa dieksperimen\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(learning_rate=INITIAL_LR, weight_decay=1e-5), # Coba AdamW\n",
    "    loss=chosen_loss,\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall')]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pelatihan Model\n",
    "Latih model dengan data generator dan callbacks yang telah disiapkan. Tambah jumlah epoch jika `EarlyStopping` menghentikan terlalu dini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\S8NRC\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Warmup learning rate set to 0.0000401\n",
      "Epoch 1/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.1087 - loss: 0.8636 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 1: val_loss improved from inf to 0.80835, saving model to best_model_optimized.keras\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 3s/step - accuracy: 0.1087 - loss: 0.8635 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.1088 - val_loss: 0.8084 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 2.0000e-04\n",
      "\n",
      "Epoch 2: Warmup learning rate set to 0.0000801\n",
      "Epoch 2/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.1145 - loss: 0.7937 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 2: val_loss improved from 0.80835 to 0.74184, saving model to best_model_optimized.keras\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 2s/step - accuracy: 0.1145 - loss: 0.7936 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.2057 - val_loss: 0.7418 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 2.0000e-04\n",
      "\n",
      "Epoch 3: Warmup learning rate set to 0.0001200\n",
      "Epoch 3/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.1223 - loss: 0.7328 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 3: val_loss improved from 0.74184 to 0.68998, saving model to best_model_optimized.keras\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 2s/step - accuracy: 0.1224 - loss: 0.7327 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.1088 - val_loss: 0.6900 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 2.0000e-04\n",
      "\n",
      "Epoch 4: Warmup learning rate set to 0.0001600\n",
      "Epoch 4/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.1344 - loss: 0.6783 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 4: val_loss improved from 0.68998 to 0.64705, saving model to best_model_optimized.keras\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 2s/step - accuracy: 0.1344 - loss: 0.6782 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.1088 - val_loss: 0.6471 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 2.0000e-04\n",
      "\n",
      "Epoch 5: Warmup learning rate set to 0.0002000\n",
      "Epoch 5/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.1469 - loss: 0.6265 - precision: 0.1106 - recall: 8.8680e-05\n",
      "Epoch 5: val_loss improved from 0.64705 to 0.60143, saving model to best_model_optimized.keras\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 2s/step - accuracy: 0.1468 - loss: 0.6264 - precision: 0.1126 - recall: 9.0338e-05 - val_accuracy: 0.1153 - val_loss: 0.6014 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 2.0000e-04\n",
      "\n",
      "Epoch 6: Learning rate set to target 0.0002000 after warmup.\n",
      "Epoch 6/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.1557 - loss: 0.5871 - precision: 0.3005 - recall: 3.8548e-04\n",
      "Epoch 6: val_loss improved from 0.60143 to 0.58826, saving model to best_model_optimized.keras\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 2s/step - accuracy: 0.1557 - loss: 0.5870 - precision: 0.3004 - recall: 3.8697e-04 - val_accuracy: 0.1088 - val_loss: 0.5883 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 2.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.1590 - loss: 0.5487 - precision: 0.1176 - recall: 2.8792e-04\n",
      "Epoch 7: val_loss improved from 0.58826 to 0.55511, saving model to best_model_optimized.keras\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 2s/step - accuracy: 0.1590 - loss: 0.5487 - precision: 0.1181 - recall: 2.9027e-04 - val_accuracy: 0.1088 - val_loss: 0.5551 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 2.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.1825 - loss: 0.5259 - precision: 0.2387 - recall: 0.0012\n",
      "Epoch 8: val_loss improved from 0.55511 to 0.48777, saving model to best_model_optimized.keras\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 2s/step - accuracy: 0.1824 - loss: 0.5259 - precision: 0.2405 - recall: 0.0013 - val_accuracy: 0.2350 - val_loss: 0.4878 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 2.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.1671 - loss: 0.5046 - precision: 0.3398 - recall: 0.0012\n",
      "Epoch 9: val_loss did not improve from 0.48777\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 2s/step - accuracy: 0.1671 - loss: 0.5046 - precision: 0.3408 - recall: 0.0012 - val_accuracy: 0.1045 - val_loss: 0.5067 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 2.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.1812 - loss: 0.4868 - precision: 0.7112 - recall: 0.0042\n",
      "Epoch 10: val_loss did not improve from 0.48777\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 2s/step - accuracy: 0.1812 - loss: 0.4868 - precision: 0.7095 - recall: 0.0042 - val_accuracy: 0.1012 - val_loss: 0.5012 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 2.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.1952 - loss: 0.4717 - precision: 0.4138 - recall: 0.0012\n",
      "Epoch 11: val_loss improved from 0.48777 to 0.43421, saving model to best_model_optimized.keras\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 2s/step - accuracy: 0.1953 - loss: 0.4716 - precision: 0.4144 - recall: 0.0012 - val_accuracy: 0.2579 - val_loss: 0.4342 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 2.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.1952 - loss: 0.4550 - precision: 0.5072 - recall: 0.0032\n",
      "Epoch 12: val_loss did not improve from 0.43421\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 2s/step - accuracy: 0.1952 - loss: 0.4549 - precision: 0.5072 - recall: 0.0032 - val_accuracy: 0.1404 - val_loss: 0.4996 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 2.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.1898 - loss: 0.4486 - precision: 0.3015 - recall: 0.0021\n",
      "Epoch 13: val_loss did not improve from 0.43421\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 2s/step - accuracy: 0.1899 - loss: 0.4485 - precision: 0.3009 - recall: 0.0021 - val_accuracy: 0.1502 - val_loss: 0.4974 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 2.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.2000 - loss: 0.4351 - precision: 0.6769 - recall: 0.0058\n",
      "Epoch 14: val_loss improved from 0.43421 to 0.40906, saving model to best_model_optimized.keras\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 2s/step - accuracy: 0.2001 - loss: 0.4350 - precision: 0.6759 - recall: 0.0057 - val_accuracy: 0.2568 - val_loss: 0.4091 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 2.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.2176 - loss: 0.4209 - precision: 0.2959 - recall: 0.0027"
     ]
    }
   ],
   "source": [
    "print(\"\\nStarting model training...\")\n",
    "EPOCHS_TO_TRAIN = 50 # EarlyStopping akan mengontrol jumlah epoch sebenarnya\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS_TO_TRAIN,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1,\n",
    "    class_weight=active_class_weights if use_class_weights_in_fit else None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualisasi Hasil Training\n",
    "Plot kurva akurasi dan loss untuk training dan validasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history_to_plot):\n",
    "    acc = history_to_plot.history.get('accuracy', [])\n",
    "    val_acc = history_to_plot.history.get('val_accuracy', [])\n",
    "    loss = history_to_plot.history.get('loss', [])\n",
    "    val_loss = history_to_plot.history.get('val_loss', [])\n",
    "\n",
    "    epochs_range = range(len(acc))\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    if acc and val_acc:\n",
    "        plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "        plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Accuracy data not available', ha='center')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    if loss and val_loss:\n",
    "        plt.plot(epochs_range, loss, label='Training Loss')\n",
    "        plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title('Training and Validation Loss')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Loss data not available', ha='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if history:\n",
    "    plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluasi Model Lebih Lanjut\n",
    "Gunakan fungsi evaluasi lanjutan untuk melihat metrik per kelas dan confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_evaluation(model_to_eval, test_gen, class_names_param):\n",
    "    test_gen.reset() # Pastikan generator kembali ke awal\n",
    "    y_pred_proba = model_to_eval.predict(test_gen, verbose=1)\n",
    "    y_pred_classes_eval = np.argmax(y_pred_proba, axis=1)\n",
    "    y_true_eval = test_gen.classes\n",
    "    \n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(y_true_eval, y_pred_classes_eval, target_names=class_names_param, zero_division=0))\n",
    "    \n",
    "    cm = confusion_matrix(y_true_eval, y_pred_classes_eval)\n",
    "    cm_normalized = confusion_matrix(y_true_eval, y_pred_classes_eval, normalize='true')\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    disp1 = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names_param)\n",
    "    disp1.plot(ax=ax1, cmap='Blues', xticks_rotation='vertical')\n",
    "    ax1.set_title(\"Raw Confusion Matrix\")\n",
    "    \n",
    "    disp2 = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=class_names_param)\n",
    "    disp2.plot(ax=ax2, cmap='Blues', xticks_rotation='vertical')\n",
    "    ax2.set_title(\"Normalized Confusion Matrix\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nPer-class Analysis:\")\n",
    "    for i, class_name_eval in enumerate(class_names_param):\n",
    "        class_mask = (y_true_eval == i)\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_accuracy = np.sum((y_pred_classes_eval == i) & class_mask) / np.sum(class_mask)\n",
    "            print(f\"{class_name_eval}: {class_accuracy:.3f} accuracy ({np.sum(class_mask)} samples)\")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=1, # Batch size 1 untuk evaluasi akurat per sampel\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Load model terbaik (jika disimpan dalam format .keras)\n",
    "# try:\n",
    "#     print(\"Loading best model from 'best_model_optimized.keras'...\")\n",
    "#     model = tf.keras.models.load_model('best_model_optimized.keras', custom_objects={'focal_loss_fixed': focal_loss() if chosen_loss.__name__ == 'focal_loss' else None})\n",
    "# except Exception as e:\n",
    "#     print(f\"Could not load .keras model: {e}. Ensure custom objects are passed if needed.\")\n",
    "#     print(\"Using the model from training history.\")\n",
    "\n",
    "print(\"\\nEvaluating model on test set...\")\n",
    "eval_results = model.evaluate(test_generator, verbose=1)\n",
    "print(f\"\\nTest Results (from model.evaluate):\")\n",
    "for metric_name, metric_value in zip(model.metrics_names, eval_results):\n",
    "    print(f\"{metric_name}: {metric_value:.4f}\")\n",
    "\n",
    "advanced_evaluation(model, test_generator, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Prediksi dengan Threshold\n",
    "Fungsi untuk prediksi gambar tunggal dengan *confidence threshold* untuk mengurangi *false positives*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_threshold(model_to_predict, image_path, img_size_param, class_names_param, conf_threshold=0.7):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Image not found at {image_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    img_resized = img.resize(img_size_param)\n",
    "    img_array = np.expand_dims(np.array(img_resized) / 255.0, axis=0)\n",
    "    \n",
    "    # Jika menggunakan preprocess_input dari Lambda layer, model sudah menanganinya.\n",
    "    # Jika tidak, dan preprocess_input belum diterapkan, lakukan di sini.\n",
    "    # img_array_preprocessed = PREPROCESS_INPUT_FUNC(img_array * 255) # Jika rescale 1./255 sudah dilakukan, kalikan 255 dulu\n",
    "    # prediction = model_to_predict.predict(img_array_preprocessed)\n",
    "    prediction = model_to_predict.predict(img_array) # Asumsi model sudah ada Lambda preprocess_input\n",
    "\n",
    "    confidence = np.max(prediction)\n",
    "    predicted_class_idx = np.argmax(prediction)\n",
    "    predicted_class = class_names_param[predicted_class_idx]\n",
    "    \n",
    "    if confidence < conf_threshold:\n",
    "        predicted_class_label = f\"Uncertain (Conf: {confidence:.2f}) - Original: {predicted_class}\"\n",
    "    else:\n",
    "        predicted_class_label = predicted_class\n",
    "\n",
    "    print(f\"Predicted Class Label: {predicted_class_label}\")\n",
    "    print(f\"Original Predicted Class (highest prob): {predicted_class}\")\n",
    "    print(f\"Confidence: {confidence:.4f}\")\n",
    "    \n",
    "    # Tampilkan probabilitas untuk semua kelas\n",
    "    # probabilities_str = \", \".join([f\"{name}: {prob:.3f}\" for name, prob in zip(class_names_param, prediction[0])])\n",
    "    # print(f\"All probabilities: {{ {probabilities_str} }}\")\n",
    "    \n",
    "    plt.imshow(img) # Tampilkan gambar asli\n",
    "    plt.title(f\"{predicted_class_label} (Confidence: {confidence:.3f})\", fontsize=10)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "# Contoh penggunaan (ganti dengan path gambar Anda yang valid)\n",
    "# Pastikan direktori dan file ada. Anda bisa mengambil path dari test_generator.filenames\n",
    "if test_generator.samples > 0:\n",
    "    example_image_path = os.path.join(test_dir, test_generator.filenames[0])\n",
    "    print(f\"\\nPredicting example image: {example_image_path}\")\n",
    "    if os.path.exists(example_image_path):\n",
    "        predict_with_threshold(model, example_image_path, img_size, class_names, conf_threshold=0.5)\n",
    "    else:\n",
    "        print(f\"Example image not found: {example_image_path}\")\n",
    "else:\n",
    "    print(\"No samples in test_generator to predict.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
